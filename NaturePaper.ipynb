{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seeds = [42, 100, 0, 10, 12, 20, 50, 9, 30, 51]\n",
    "run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'meganorm-mznasrabadi'\n",
    "datasets = {\n",
    "    'BTNRH': {\n",
    "        'base_dir': '/project/meganorm/Data/BTNRH/BIDS',\n",
    "        'task': \"task-rest\",\n",
    "        \"ending\" : \"meg.fif\"\n",
    "        },\n",
    "    'CAMCAN': {\n",
    "        'base_dir': '/project/meganorm/Data/camcan/BIDS',\n",
    "        'task': \"task-rest\",\n",
    "        \"ending\" : \"meg.fif\"\n",
    "        },\n",
    "    'NIMH': {\n",
    "        'base_dir': '/project/meganorm/Data/NIMH',\n",
    "        'task': \"task-rest\",\n",
    "        \"ending\" : \"meg.ds\"\n",
    "        },\n",
    "    'OMEGA': {\n",
    "        'base_dir': '/project/meganorm/Data/Omega',\n",
    "        'task': \"task-rest\",\n",
    "        \"ending\" : \"meg.ds\"\n",
    "        },\n",
    "    'HCP': {\n",
    "        'base_dir': '/project/meganorm/Data/HCP',\n",
    "        'task': \"\",\n",
    "        \"ending\" : \"4-Restin/4D\"\n",
    "        },\n",
    "    'MOUS': {\n",
    "        'base_dir': '/project/meganorm/Data/MOUS',\n",
    "        'task': \"task-rest\",\n",
    "        \"ending\" : \"meg.ds\"\n",
    "    }\n",
    "    }\n",
    "\n",
    "package_path = f'/home/{username}/MEGaNorm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(package_path)\n",
    "from utils.parallel import submit_jobs, check_jobs_status, collect_results\n",
    "from utils.nm import hbr_data_split, estimate_centiles, evaluate_mace, shapiro_stat, abnormal_probability\n",
    "from plots.plots import plot_nm_range_site2, plot_comparison, plot_neurooscillochart, plot_age_dist2, plot_growthcharts, plot_quantile_gauge, box_plot_auc, z_scores_scatter_plot, plot_metrics\n",
    "from utils.nm import model_quantile_evaluation, calculate_oscilochart, prepare_prediction_data, cal_stats_for_gauge, aggregate_metrics_across_runs\n",
    "from utils.IO import merge_datasets_with_regex, merge_fidp_demo, merge_datasets_with_glob\n",
    "import pandas as pd\n",
    "import json\n",
    "from pcntoolkit.normative_parallel import execute_nm, rerun_nm, collect_nm\n",
    "import warnings\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pcntoolkit.util.utils import z_to_abnormal_p, anomaly_detection_auc\n",
    "from scipy.stats import false_discovery_control\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_config(project, path=None):\n",
    "\n",
    "    # preprocess configurations =================================================\n",
    "    # downsample data\n",
    "    config = dict()\n",
    "\n",
    "    # You could also set layout to None to have high \n",
    "    # choices: all, lobe, None\n",
    "    config[\"which_layout\"] = \"all\"\n",
    "\n",
    "    # which sensor type should be used\n",
    "    # choices: meg, mag, grad, eeg, opm\n",
    "    config[\"which_sensor\"] = \"meg\"\n",
    "    # config['fs'] = 1000\n",
    "\n",
    "    # ICA configuration\n",
    "    config['ica_n_component'] = 30\n",
    "    config['ica_max_iter'] = 800\n",
    "    config['ica_method'] = \"fastica\"\n",
    "\n",
    "    # lower and upper cutoff frequencies in a bandpass filter\n",
    "    config['cutoffFreqLow'] = 1\n",
    "    config['cutoffFreqHigh'] = 45\n",
    "\n",
    "    config[\"resampling_rate\"] = 1000\n",
    "    config[\"digital_filter\"] = True\n",
    "    config[\"notch_filter\"] = False\n",
    "\n",
    "    config[\"apply_ica\"] = True\n",
    "\n",
    "    config[\"auto_ica_corr_thr\"] = 0.9\n",
    "\n",
    "    # options are \"average\", \"REST\", and None \n",
    "    config[\"rereference_method\"]= \"average\"\n",
    "\n",
    "    # variance threshold across time\n",
    "    config[\"mag_var_threshold\"] = 4e-12\n",
    "    config[\"grad_var_threshold\"] = 4000e-13\n",
    "    config[\"eeg_var_threshold\"] = 40e-6\n",
    "    # flatness threshold across time\n",
    "    config[\"mag_flat_threshold\"] = 10e-15\n",
    "    config[\"grad_flat_threshold\"] = 10e-15\n",
    "    config[\"eeg_flat_threshold\"] = 40e-6\n",
    "    # variance thershold across channels\n",
    "    config[\"zscore_std_thresh\"] = 15 # change this\n",
    "\n",
    "    # segmentation ==============================================\n",
    "    #start time of the raw data to use in seconds, this is to avoid possible eye blinks in close-eyed resting state. \n",
    "    config['segments_tmin'] = 20\n",
    "    # end time of the raw data to use in seconds, this is to avoid possible eye blinks in close-eyed resting state.\n",
    "    config['segments_tmax'] = -20\n",
    "    # length of MEG segments in seconds\n",
    "    config['segments_length'] = 10\n",
    "    # amount of overlap between MEG sigals in seconds\n",
    "    config['segments_overlap'] = 2\n",
    "\n",
    "    # PSD ==============================================\n",
    "    # Spectral estimation method\n",
    "    config['psd_method'] = \"welch\"\n",
    "    # amount of overlap between windows in Welch's method\n",
    "    config['psd_n_overlap'] = 1\n",
    "    config['psd_n_fft'] = 2\n",
    "    # number of samples in psd\n",
    "    config[\"psd_n_per_seg\"] = 2\n",
    "\n",
    "    # fooof analysis configurations ==============================================\n",
    "    # Desired frequency range to run FOOOF\n",
    "    config['fooof_freqRangeLow'] = 3\n",
    "    config['fooof_freqRangeHigh'] = 40\n",
    "    # which mode should be used for fitting; choices (knee, fixed)\n",
    "    config[\"aperiodic_mode\"] = \"knee\"\n",
    "    # minimum acceptable peak width in fooof analysis\n",
    "    config[\"fooof_peak_width_limits\"] = [1.0, 12.0]\n",
    "    #Absolute threshold for detecting peaks\n",
    "    config['fooof_min_peak_height'] = 0\n",
    "    #Relative threshold for detecting peaks\n",
    "    config['fooof_peak_threshold'] = 2\n",
    "\n",
    "    # feature extraction ==========================================================\n",
    "    # Define frequency bands\n",
    "    config['freq_bands'] = {\n",
    "                            'Theta': (3, 8),\n",
    "                            'Alpha': (8, 13),\n",
    "                            'Beta': (13, 30),\n",
    "                            'Gamma': (30, 40),\n",
    "                            # 'Broadband': (3, 40)\n",
    "                            }\n",
    "\n",
    "    # Define individualized frequency range over main peaks in each freq band\n",
    "    config['individualized_band_ranges'] = { \n",
    "                                            'Theta': (-2, 3),\n",
    "                                            'Alpha': (-2, 3), # change to (-4,2)\n",
    "                                            'Beta': (-8, 9),\n",
    "                                            'Gamma': (-5, 5)\n",
    "                                            }\n",
    "\n",
    "    # least acceptable R squred of fitted models\n",
    "    config['min_r_squared'] = 0.9 \n",
    " \n",
    "    config['feature_categories'] = {\n",
    "                                    \"Offset\":True,\n",
    "                                    \"Exponent\":True,\n",
    "                                    \"Peak_Center\":True,\n",
    "                                    \"Peak_Power\":True,\n",
    "                                    \"Peak_Width\":True,\n",
    "                                    \"Adjusted_Canonical_Relative_Power\":True, \n",
    "                                    \"Adjusted_Canonical_Absolute_Power\":True,\n",
    "                                    \"Adjusted_Individualized_Relative_Power\":True,\n",
    "                                    \"Adjusted_Individualized_Absolute_Power\":True,\n",
    "                                    \"OriginalPSD_Canonical_Relative_Power\":True, \n",
    "                                    \"OriginalPSD_Canonical_Absolute_Power\":True,\n",
    "                                    \"OriginalPSD_Individualized_Relative_Power\":True,\n",
    "                                    \"OriginalPSD_Individualized_Absolute_Power\":True,\n",
    "                                    }\n",
    "    \n",
    "    config[\"fooof_res_save_path\"] = None\n",
    "\n",
    "    config[\"random_state\"] = 42\n",
    "\n",
    "    if path is not None:\n",
    "        out_file = open(os.path.join(path, project + \".json\"), \"w\") \n",
    "        json.dump(config, out_file, indent = 6) \n",
    "        out_file.close()\n",
    "\n",
    "    return config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"test_feature_ex\"\n",
    "\n",
    "project_dir = f'/home/{username}/Results/{project}/'\n",
    "\n",
    "mainParallel_path = os.path.join(package_path, 'src', 'mainParallel.py')\n",
    "\n",
    "features_dir = os.path.join(project_dir, 'Features')\n",
    "features_log_path = os.path.join(features_dir, 'log')\n",
    "features_temp_path = os.path.join(features_dir,'temp')\n",
    "pics_dir = os.path.join(project_dir, \"pics\")\n",
    "\n",
    "nm_processing_dir = os.path.join(project_dir, 'NM', 'Run_' + str(run))\n",
    "\n",
    "job_configs = {'log_path':features_log_path, 'module':'mne', 'time':'1:00:00', 'memory':'20GB', \n",
    "                'partition':'normal', 'core':1, 'node':1, 'batch_file_name':'batch_job'}\n",
    "\n",
    "if not os.path.isdir(features_log_path):\n",
    "    os.makedirs(features_log_path)\n",
    "\n",
    "if not os.path.isdir(features_temp_path):\n",
    "    os.makedirs(features_temp_path)\n",
    "    \n",
    "if not os.path.isdir(nm_processing_dir):\n",
    "    os.makedirs(nm_processing_dir)\n",
    "\n",
    "if not os.path.isdir(pics_dir):\n",
    "    os.makedirs(pics_dir)\n",
    "    \n",
    "configs = make_config(project, project_dir)\n",
    "\n",
    "subjects = merge_datasets_with_glob(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f-IDPs extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel feature extraction  \n",
    "\n",
    "# Running Jobs\n",
    "start_time = submit_jobs(mainParallel_path, features_dir, subjects, \n",
    "                features_temp_path, job_configs=job_configs, config_file=os.path.join(project_dir, project+'.json'))\n",
    "# Checking jobs\n",
    "failed_jobs = check_jobs_status(username, start_time)\n",
    "\n",
    "falied_subjects = {failed_job:subjects[failed_job] for failed_job in failed_jobs}\n",
    "\n",
    "while len(failed_jobs)>0:\n",
    "    # Re-running Jobs\n",
    "    start_time = submit_jobs(mainParallel_path, features_dir, falied_subjects, \n",
    "                features_temp_path, job_configs=job_configs, config_file=os.path.join(project_dir, project+'.json'))\n",
    "    # Checking jobs\n",
    "    failed_jobs = check_jobs_status(username, start_time)\n",
    "\n",
    "collect_results(features_dir, subjects, features_temp_path, file_name='all_features', clean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data preparation for Normative Modeling\n",
    "data_base_dirs = [values[\"base_dir\"] for values in datasets.values()]\n",
    "dataset_names = list(datasets.keys())\n",
    "merged_data, data_patient = merge_fidp_demo(data_base_dirs, features_dir, dataset_names, include_patients=False, diagnosis=\"parkinson\")\n",
    "\n",
    "biomarker_num = hbr_data_split(merged_data, nm_processing_dir, drop_nans=True, batch_effects=['sex', 'site'], random_seed=random_seeds[run], train_split=0.5)\n",
    "\n",
    "biomarker_names = list(merged_data.columns[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 4\n",
    "print(\"Dataset name:\", list(datasets.keys())[site])\n",
    "print(\"size: \", merged_data[merged_data.site==site].shape[0], \"participants\")\n",
    "print(\"mean age: \", merged_data[merged_data.site==site][\"age\"].mean())\n",
    "print(\"std age: \", merged_data[merged_data.site==site][\"age\"].std())\n",
    "print(\"female num: \", merged_data[np.logical_and(merged_data.site==site, merged_data.sex==1)].shape[0], \"participants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NM configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting up NM configs\n",
    "\n",
    "python_path = '/project/meganorm/Software/Miniconda3/envs/mne/bin/python' \n",
    "\n",
    "hbr_configs = {\n",
    "                'homo_Gaussian_linear':{'model_type':'linear', 'likelihood':'Normal', 'linear_sigma':'False',\n",
    "                                        'random_slope_mu':'False', 'linear_epsilon':'False', 'linear_delta':'False'}, \n",
    "                'homo_Gaussian_bspline':{'model_type':'bspline', 'likelihood':'Normal', 'linear_sigma':'False',\n",
    "                                        'random_slope_mu':'False', 'linear_epsilon':'False', 'linear_delta':'False'}, \n",
    "                'homo_SHASH_linear':{'model_type':'linear', 'likelihood':'SHASHb', 'linear_sigma':'False',\n",
    "                                    'random_slope_mu':'False', 'linear_epsilon':'False', 'linear_delta':'False'}, \n",
    "                'homo_SHASH_bspline':{'model_type':'bspline', 'likelihood':'SHASHb', 'linear_sigma':'False',\n",
    "                                    'random_slope_mu':'False', 'linear_epsilon':'False', 'linear_delta':'False'}, \n",
    "                'hetero_Gaussian_linear':{'model_type':'linear', 'likelihood':'Normal', 'linear_sigma':'True',\n",
    "                                        'random_slope_mu':'False', 'linear_epsilon':'False', 'linear_delta':'False'},\n",
    "                'hetero_Gaussian_bspline':{'model_type':'bspline', 'likelihood':'Normal', 'linear_sigma':'True',\n",
    "                                        'random_slope_mu':'False', 'linear_epsilon':'False', 'linear_delta':'False'},\n",
    "                'hetero_SHASH_linear':{'model_type':'linear', 'likelihood':'SHASHb', 'linear_sigma':'True',\n",
    "                                    'random_slope_mu':'False', 'linear_epsilon':'True', 'linear_delta':'True'},\n",
    "                'hetero_SHASH_bspline':{'model_type':'bspline', 'likelihood':'SHASHb', 'linear_sigma':'True',\n",
    "                                        'random_slope_mu':'False', 'linear_epsilon':'True', 'linear_delta':'True'},\n",
    "            }\n",
    "\n",
    "inscaler='None' \n",
    "outscaler='None' \n",
    "batch_size = 1\n",
    "outputsuffix = '_estimate'\n",
    "\n",
    "respfile = os.path.join(nm_processing_dir, 'y_train.pkl')\n",
    "covfile = os.path.join(nm_processing_dir, 'x_train.pkl')\n",
    "\n",
    "testrespfile_path = os.path.join(nm_processing_dir, 'y_test.pkl')\n",
    "testcovfile_path = os.path.join(nm_processing_dir, 'x_test.pkl')\n",
    "\n",
    "trbefile = os.path.join(nm_processing_dir, 'b_train.pkl')\n",
    "tsbefile = os.path.join(nm_processing_dir, 'b_test.pkl')\n",
    "\n",
    "memory = '2gb'\n",
    "duration = '5:00:00'\n",
    "cluster_spec = 'slurm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running NM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for method in hbr_configs.keys():\n",
    "method = 'hetero_SHASH_bspline'\n",
    "processing_dir = os.path.join(nm_processing_dir, method) + '/'\n",
    "nm_log_path = os.path.join(processing_dir, 'log') + '/'\n",
    "\n",
    "if not os.path.isdir(processing_dir):\n",
    "    os.makedirs(processing_dir)\n",
    "if not os.path.isdir(nm_log_path):\n",
    "    os.makedirs(nm_log_path)\n",
    "\n",
    "execute_nm(processing_dir, python_path,\n",
    "            'NM', covfile, respfile, batch_size, memory, duration, alg='hbr', \n",
    "            log_path=nm_log_path, binary=True, testcovfile_path=testcovfile_path, \n",
    "            testrespfile_path=testrespfile_path,trbefile=trbefile, tsbefile=tsbefile, \n",
    "            model_type=hbr_configs[method]['model_type'], likelihood=hbr_configs[method]['likelihood'],  \n",
    "            linear_sigma=hbr_configs[method]['linear_sigma'], random_slope_mu=hbr_configs[method]['random_slope_mu'],\n",
    "            linear_epsilon=hbr_configs[method]['linear_epsilon'], linear_delta=hbr_configs[method]['linear_delta'], \n",
    "            savemodel='True', inscaler=inscaler, outscaler=outscaler, outputsuffix=outputsuffix, \n",
    "            interactive='auto', cluster_spec=cluster_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect_nm(processing_dir, \"NM\", collect=True, binary=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_values = aggregate_metrics_across_runs(nm_processing_dir, method, biomarker_names, testcovfile_path, \n",
    "                              testrespfile_path, tsbefile,)\n",
    "metrics_summary_path = os.path.join(processing_dir, \"metrics_summary.pkl\")\n",
    "with open(metrics_summary_path, \"wb\") as file:\n",
    "    pickle.dump(metrics_values, file)\n",
    "\n",
    "plot_metrics(metrics_summary_path, \n",
    "            biomarker_names, \n",
    "            feature_new_name=[\"Theta\", \"Alpha\", \n",
    "                            \"Beta\", \"Gamma\"], \n",
    "            save_path=pics_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluating quantiles using MACE\n",
    "\n",
    "mace, best_models, bio_ids = model_quantile_evaluation(hbr_configs, nm_processing_dir, testcovfile_path, \n",
    "                              testrespfile_path, tsbefile, biomarker_num, plot=False, outputsuffix='estimate')\n",
    "\n",
    "plot_comparison(nm_processing_dir, hbr_configs, biomarker_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NM ranges with markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ranges\n",
    "# # for config in hbr_configs.keys():\n",
    "processing_path = os.path.join(nm_processing_dir, method)\n",
    "\n",
    "q = estimate_centiles(processing_path, biomarker_num, quantiles=[0.05, 0.25, 0.5, 0.75, 0.95],\n",
    "                        batch_map={0:{'Male':0, 'Female':1}, 1:{'BTNRH':0, 'CAMCAN':1, \"NIMH\":2, \"OMEGA\":3, \"HCP\":4}}, \n",
    "                        age_range=[6, 80])\n",
    "plot_nm_range_site2(processing_path, nm_processing_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age distribution (plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_age_dist2(merged_data, site_names=list(datasets.keys()), save_path=pics_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_seeds = [0]\n",
    "for i in range(len(random_seeds)):\n",
    "\n",
    "    nm_processing_dir_temp = nm_processing_dir.replace(\"Run_0\", f\"Run_{i}\")\n",
    "    processing_dir_temp = processing_dir.replace(\"Run_0\", f\"Run_{i}\")\n",
    "\n",
    "    prefix = \"clinicalpredict_\"\n",
    "    prepare_prediction_data(data_patient.drop('diagnosis', axis=1),\n",
    "                                nm_processing_dir_temp, \n",
    "                                drop_nans=True, \n",
    "                                batch_effects=['sex', 'site'], \n",
    "                                prefix=prefix)\n",
    "\n",
    "    testrespfile_path = os.path.join(nm_processing_dir_temp, prefix + 'y_test.pkl')\n",
    "    testcovfile_path = os.path.join(nm_processing_dir_temp, prefix + 'x_test.pkl')\n",
    "    tsbefile = os.path.join(nm_processing_dir_temp, prefix + 'b_test.pkl')\n",
    "\n",
    "    execute_nm(processing_dir_temp, python_path,\n",
    "            'NM', testcovfile_path, testrespfile_path, batch_size, memory, duration, alg='hbr', \n",
    "            log_path=nm_log_path, binary=True, tsbefile=tsbefile, func=\"predict\", \n",
    "            model_type=hbr_configs[method]['model_type'], likelihood=hbr_configs[method]['likelihood'],  \n",
    "            linear_sigma=hbr_configs[method]['linear_sigma'], random_slope_mu=hbr_configs[method]['random_slope_mu'],\n",
    "            linear_epsilon=hbr_configs[method]['linear_epsilon'], linear_delta=hbr_configs[method]['linear_delta'], \n",
    "            savemodel='True', inscaler=inscaler, outscaler=outscaler, outputsuffix=\"clinicalpredict\", inputsuffix=outputsuffix,\n",
    "            interactive='auto', cluster_spec=cluster_spec, nuts_sampler=\"nutpie\", n_cores_per_batch=\"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abnormal probability index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_id = 3\n",
    "p_vals, aucs = [], []\n",
    "\n",
    "for i in range(len(random_seeds)):\n",
    "\n",
    "    nm_processing_dir_temp = nm_processing_dir.replace(\"Run_0\", f\"Run_{i}\")\n",
    "    processing_dir_temp = processing_dir.replace(\"Run_0\", f\"Run_{i}\")\n",
    "\n",
    "    p_val, auc = abnormal_probability(processing_dir_temp,\n",
    "                                    nm_processing_dir_temp, \n",
    "                                    site_id,\n",
    "                                    n_permutation=1000)\n",
    "    \n",
    "    p_vals.append(p_val); aucs.append(auc)\n",
    "\n",
    "p_vals = pd.DataFrame(np.vstack(p_vals))\n",
    "aucs = pd.DataFrame(np.vstack(aucs))\n",
    "\n",
    "aucs.columns = [\"Theta\", \"Alpha\", \"Beta\", \"Gamma\", \"averaged\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot_auc(aucs, save_path=pics_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter plot of Z-scores for patients with parkinson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(processing_dir, \"Z_clinicalpredict.pkl\"), \"rb\") as file:\n",
    "    z_patient = pickle.load(file)\n",
    "\n",
    "z_patient.index = data_patient.index\n",
    "z_patient.columns = biomarker_names\n",
    "\n",
    "\n",
    "z_scores_scatter_plot(X = list(z_patient.loc[:, \"Adjusted_Canonical_Relative_PowerTheta_all\"]),\n",
    "                    Y = list(z_patient.loc[:, \"Adjusted_Canonical_Relative_PowerBeta_all\"]),\n",
    "                    bands_name=[\"theta\", \"beta\"], \n",
    "                    thr=0.68,\n",
    "                    save_path=pics_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INOCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_path = f\"/home/{username}/Results/{project}/NM/Run_0/hetero_SHASH_bspline/Quantiles_estimate.pkl\"\n",
    "\n",
    "sub_index = \"sub-042\"\n",
    "statistics = cal_stats_for_gauge(q_path, biomarker_names, \n",
    "                                 site_id=merged_data.loc[sub_index][\"site\"], \n",
    "                                 gender_id=merged_data.loc[sub_index][\"sex\"], \n",
    "                                 age=merged_data.loc[sub_index][\"age\"]*100)\n",
    "\n",
    "new_names = [\"Theta\", \"Alpha\", \"Beta\", \"Gamma\"]\n",
    "\n",
    "for i, name in enumerate(biomarker_names):\n",
    "\n",
    "    if new_names[i] == \"Gamma\": max_value=0.2\n",
    "    else: max_value=1\n",
    "\n",
    "    plot_quantile_gauge(merged_data.loc[sub_index, name],\n",
    "                        statistics[name][1],\n",
    "                        statistics[name][3],\n",
    "                        statistics[name][0],\n",
    "                        statistics[name][4],\n",
    "                        statistics[name][2],\n",
    "                        title=\"\",\n",
    "                        max_value=max_value,\n",
    "                        show_legend=False,\n",
    "                        bio_name=new_names[i],\n",
    "                        save_path=pics_dir\n",
    "                        )\n",
    "    \n",
    "# sub-PD1674\n",
    "# sub-PD1551\n",
    "# sub-PD1517\n",
    "# sub-MNI0079\n",
    "# sub-PD0512\n",
    "# sub-PD1487"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PNOCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculateing Oscilograms\n",
    "\n",
    "gender_ids = {'Male':0, 'Female':1}\n",
    "frequency_band_model_ids = {'Theta':0, 'Alpha':1, 'Beta':2, 'Gamma':3}\n",
    "quantiles_path= os.path.join(processing_dir, 'Quantiles_estimate.pkl')\n",
    "oscilograms, age_slices = calculate_oscilochart(quantiles_path, gender_ids, frequency_band_model_ids)\n",
    "\n",
    "plot_neurooscillochart(oscilograms, age_slices, save_path=pics_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot growthchart using all of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_growthcharts(processing_dir, \n",
    "                  idp_indices=list(range(len(new_names))), \n",
    "                  idp_names = new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcntoolkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
